{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "703f877b",
   "metadata": {},
   "source": [
    "References:\n",
    "- https://github.com/pyg-team/pytorch_geometric\n",
    "- https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html\n",
    "- https://colab.research.google.com/drive/14OvFnAXggxB8vM4e8vSURUp1TaKnovzX?usp=sharing#scrollTo=imGrKO5YH11-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee09c69",
   "metadata": {},
   "source": [
    "KNOWN_BUGS:\n",
    "- Component Layer doesn't currently account for through hole components w/ pads on the opposite side. This is because I am throwing away pad info and just using component outline info.\n",
    "- Board Outline is currently ignored\n",
    "- Not currently exporting/using non-designator text\n",
    "\n",
    "TODO:\n",
    "- Update categorical handling to automatically handle any number of categorical features\n",
    "- If I get this architecture making descent predictions, I need to predict silkscreen sizes and not just have them be static\n",
    "  \n",
    "NOTES:\n",
    "- Cadence doesn't seem to have any standard for naming silk layers or artwork groups. For example silkscreen top could be used as the assembly top output to gerber or vice versa and everything in between. Also, there are 10s of silkscreen layer options. I am currently assuming the valid silk layers by searching the artwork groups for \"silk\" in the name or a couple of other variations that i've seen, but this is by no means a catch all solution and may need to be manually edited for each board export.\n",
    "- Currently not using slk-arc in training (I don't see this being necessary until predictions become more accurate. I don't think this is a major factor holding back good predictions). I do have it's data preprocessing in this notebook, but it is not used in training.\n",
    "- I created a tool embedding to feed to the network (cadence/altium), but it will still get created if only one tool is used.\n",
    "- I was using PyGeometric's train/val/test mask attribute, but had to remove it because if you use DataLoader w/ mask, sometimes you will get batches of data w/ the entire mask set to False which will give nan predictions.\n",
    "\n",
    "Tools:\n",
    "- Altium export/import scripts tested w/ 22.5.1\n",
    "- Cadence export script tested w/ PCB Editor v17.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3af4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './models')\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from time import time\n",
    "from collections import Counter\n",
    "\n",
    "# Add project directories not tracked by git\n",
    "prj_dirs = ['models']\n",
    "for d in ['train','test','predictions']:\n",
    "    subdir = os.path.join('data',d)\n",
    "    prj_dirs += [subdir]\n",
    "    \n",
    "for d in prj_dirs:\n",
    "    if not(os.path.exists(d)):\n",
    "        os.mkdir(d)\n",
    "\n",
    "DATA_DIR = os.path.join('data','train')\n",
    "TEST_DIR = os.path.join('data','test')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792f466b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = [os.path.join(DATA_DIR, f) for f in os.listdir(DATA_DIR) if f.endswith('.csv')]\n",
    "assert len(data_files) > 0, f'Add csv datasets to {DATA_DIR} before running.'\n",
    "test_files = [os.path.join(TEST_DIR, f) for f in os.listdir(TEST_DIR) if f.endswith('.csv')]\n",
    "data_files += test_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3302ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720ac8c0",
   "metadata": {},
   "source": [
    "### Data Description:\n",
    "\n",
    "| Column     | Type | Description |\n",
    "| ----------- | ----------- | ----------- |\n",
    "| Tool      | N/A     | altium/cadence|\n",
    "| Type      | N/A       | Features extracted from type Silk or Pin (x/y/L/R/T/B). If Pin Type, features may be extracted from parent Component |\n",
    "| Designator      | Component       | Parent Component Designator |\n",
    "| x/y      | Component or Silk       | Origin Coords |\n",
    "| L/R/T/B      | Component or Silk       | Bounding Rectangle (x1,x2,y1,y2 for tracks/arcs) |\n",
    "| Rotation      | Component or Silk       | Degrees |\n",
    "| Layer      | Component      | Top or Bottom |\n",
    "| Info      | Type Specific      | Delimited list of extra info specific to Type |\n",
    "| Board      | N/A     | This is not imported data. This is an index of each board/dataset |\n",
    "\n",
    "### Info Column Specific Data\n",
    "\n",
    "| Tool  | Info Column     | Type | Description |\n",
    "| ----- | ----------- | ----------- | ----------- |\n",
    "| Cadence | OriginalLayer      | All     | Original Cadence Layer Name before filtered to Top/Bottom |\n",
    "| Altium | PinName      | Pin     | Pin Name |\n",
    "| Cadence | PinNumber      | Pin     | Pin Number |\n",
    "| Cadence | PadstackName      | Pin     | Padstack Name |\n",
    "| Cadence | PinRotation      | Pin     | Rotation in Degrees |\n",
    "| Cadence | PinRelativeRotation      | Pin     | Rotation in Degrees Relative to Component |\n",
    "| Cadence | IsThrough      | Pin     | Is a Through Hole pin |\n",
    "| Both | NetName      | Pin     | Pin's Net Name |\n",
    "| Both | PinX/Y      | Pin     | Pin's Origin Coords |\n",
    "| Both | Width      | Track/Arc     | Track/Arc Width |\n",
    "| Altium | Length      | Track     | Track Length |\n",
    "| Both | Radius      | Arc     | Radius in mils |\n",
    "| Altium | StartAngle      | Arc     | Start Angle |\n",
    "| Altium | EndAngle      | Arc     | End Angle |\n",
    "| Cadence | IsCircle      | Arc     | CIRCLE/UNCLOSED_ARC |\n",
    "| Cadence | IsClockwise      | Arc     | TRUE/FALSE |\n",
    "| Both | InComponent      | Track/Arc     | Object Part of Component Footprint (0=False,-1=True) |\n",
    "| Cadence | LineType      | Track     | vertical/horizontal/odd |\n",
    "| Cadence | Justify      | Des     | Text Justification (CENTER,) |\n",
    "| Cadence | IsMirror      | Des     | Text is mirrored (TRUE/FALSE) |\n",
    "\n",
    "Notes: \n",
    "- units in mils\n",
    "- Pin Type varies for each row (Pin/Net/PinX/PinY), but Component data is the duplicate info across these rows (x/y/L/R/T/B/Rotation/Layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57030a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all csv datasets\n",
    "df_list = []\n",
    "for i, file in enumerate(data_files):\n",
    "    df = pd.read_csv(file)\n",
    "    df['Board'] = i\n",
    "    \n",
    "    # Verification\n",
    "    assert all([t in set(df.Type) for t in ['pin', 'slk-des']]), 'Missing data type.'\n",
    "    \n",
    "    df_list.append(df)\n",
    "\n",
    "df = pd.concat(df_list, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb466f",
   "metadata": {},
   "source": [
    "### Separate Tracks/Arcs from the main dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4514a1fa",
   "metadata": {},
   "source": [
    "### Split Info Column in to Multiple Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1374ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = df.loc[df.Type.isin(['slk-trk','slk-arc'])] # Track/Arc Dataset\n",
    "df = df.loc[~df.Type.isin(['slk-trk','slk-arc'])] # Main Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3a3272",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_info(df):\n",
    "    info_cols = set([i.split(':')[0] for row in df.Info for i in row.split(';') if i.split(':')[0] != ''])\n",
    "    print(f'Info Columns: {info_cols}')\n",
    "\n",
    "    def get_info(col, info, D=';'):\n",
    "        info_dict = {i.split(':')[0]:i.split(':')[-1] for i in info.split(D)}\n",
    "        return info_dict.get(col, 'NA')\n",
    "\n",
    "    for c in info_cols:\n",
    "        df[c] = df.Info.apply(lambda i: get_info(c, i))\n",
    "\n",
    "    df.drop(columns=['Info'], inplace=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf778427",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = split_info(dt)\n",
    "    \n",
    "dt.drop(columns=['Rotation'], inplace=True)\n",
    "dt.rename(columns={'L':'x1','R':'x2','T':'y1','B':'y2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cd0dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df.Info.isna(), 'Info'] = '' # Convert NaN to empty\n",
    "# tmp = split_info(df.copy()) # Don't need the info cols for df, so just hold in tmp dataframe\n",
    "\n",
    "# print('\\nVisually verify each original layer is grouped in to the correct top or bottom layer group.')\n",
    "# print('If it gets it wrong, this is defined in the cadence export script')\n",
    "# tmp.loc[(tmp.Tool=='cadence'), ['Layer', 'OriginalLayer']].drop_duplicates(subset=['Layer','OriginalLayer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d9d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab0a21c",
   "metadata": {},
   "source": [
    "### Drop unmanufacturable track widths\n",
    "note: I don't know how cadence handles silk track visibility in gerbers because not all silk tracks make it to the gerber. So I'm currently assuming anything less than 4 mils is not in the final gerber."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5d260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_SLK_TRK_WIDTH = 4 # mils\n",
    "#dt = dt.loc[(dt.Width.astype(float) > MIN_SLK_TRK_WIDTH) | (dt.Width.astype(float) == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2741c0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = len(df)\n",
    "\n",
    "# Drop duplicated silkscreen designators (I dont know a clean way to know which silk to use if duplicates exist)\n",
    "df = df[~((df.Type == 'slk-des') & df.duplicated(subset=['Board','Type','Designator','Layer']))]\n",
    "print(f'Dropped {orig_len - len(df)} duplicates.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db853b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "assert len(df.loc[(df.Type=='slk-des') & (df.duplicated(subset=['Board','Type','Designator','Layer']))]) == 0, \"Duplicate Silk Rows\"\n",
    "\n",
    "row_cnts = df.groupby(by=['Board']).Designator.count().tolist()\n",
    "if len(set(df.Board)) > 1:\n",
    "    assert all([row_cnts[0] == r for r in row_cnts]) == False, 'Each board has the same number of data rows, something is probably wrong with the input data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30583a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm not going to use Pin Info for this notebook (will merge duplicate pin info in to component info)\n",
    "df = df[~((df.Type == 'pin') & df.duplicated(subset=['Board','Type','Designator','Layer']))]\n",
    "\n",
    "df.drop(columns=['Info'], inplace=True)\n",
    "df.loc[df.Type == 'pin', 'Type'] = 'cmp'\n",
    "\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627a026",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0ffdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_feature_distribution(feature_data, idx_to_key=None):\n",
    "    probs_dict = {}\n",
    "    for i, v in enumerate(list(set(feature_data))):\n",
    "        probs_dict[v] = sum(feature_data == v)/len(feature_data)\n",
    "\n",
    "    if idx_to_key:\n",
    "        keys = [str(idx_to_key[i]) for i in probs_dict.keys()]\n",
    "    else:\n",
    "        keys = [str(k) for k in probs_dict.keys()]\n",
    "        \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(keys, probs_dict.values())\n",
    "    plt.show()\n",
    "\n",
    "def categorical_encoding(data_list, thresh=0.01, drop=False):\n",
    "    \"\"\"\n",
    "    drop: False='data values below thresh will be added to OTHER key.'\n",
    "          True='data values below thresh will be excluded from encoding'\n",
    "    \"\"\"\n",
    "    cnt = Counter(data_list) \n",
    "    valid = [k for k, v in cnt.most_common() if v/sum(cnt.values()) > thresh]\n",
    "    \n",
    "    if drop:\n",
    "        key_to_idx = {c:i for i, c in enumerate(sorted(valid), 0)}\n",
    "    else:\n",
    "        key_to_idx = {} if len(cnt) == len(valid) else {'OTHER': 0}\n",
    "        key_to_idx.update({c:i for i, c in enumerate(sorted(valid), len(key_to_idx))})\n",
    "    \n",
    "    idx_to_key = {v:k for k, v in key_to_idx.items()}\n",
    "    \n",
    "    if drop:\n",
    "        encoding = [key_to_idx[d] for d in data_list if d in key_to_idx.keys()]\n",
    "    else:\n",
    "        encoding = [key_to_idx.get(d, 0) for d in data_list]\n",
    "    \n",
    "    return encoding, key_to_idx, idx_to_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d10b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, tool_to_class, class_to_tool = categorical_encoding(df.Tool)\n",
    "df['X_Tool'] = encoding\n",
    "df.drop(columns=['Tool'], inplace=True)\n",
    "\n",
    "dt['X_Tool'] = [tool_to_class[t] for t in dt.Tool]\n",
    "dt.drop(columns=['Tool'], inplace=True)\n",
    "\n",
    "# Verification\n",
    "assert set(df.X_Tool) == set(tool_to_class.values()), 'encoding mismatch'\n",
    "\n",
    "print_feature_distribution(df.X_Tool, idx_to_key=class_to_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d603348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a categorical feature from the refdes alphabetic prefix\n",
    "\n",
    "def create_refdes_class(refdes):\n",
    "    return ''.join([c for c in refdes if c.isalpha()])\n",
    "\n",
    "df['R_DES'] = df.Designator.apply(create_refdes_class) \n",
    "print_feature_distribution(df.R_DES)\n",
    "\n",
    "df['X_DES'], refdes_to_class, class_to_refdes = categorical_encoding(df.R_DES)\n",
    "df.drop(columns='R_DES', inplace=True)\n",
    "\n",
    "# Verification\n",
    "assert set(df.X_DES) == set(refdes_to_class.values()), 'encoding mismatch'\n",
    "\n",
    "print_feature_distribution(df.X_DES, idx_to_key=class_to_refdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae78558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track & Arc Designator Feature\n",
    "dt.loc[dt.Designator.isna(), 'Designator'] = 'OTHER'\n",
    "dt['R_DES'] = dt.Designator.apply(create_refdes_class) \n",
    "dt['X_DES'] = [refdes_to_class.get(r, 0) for r in dt.R_DES]\n",
    "dt.drop(columns='R_DES', inplace=True)\n",
    "\n",
    "print_feature_distribution(dt.X_DES, idx_to_key=class_to_refdes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4be23bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_feature_distribution(df.Layer)\n",
    "\n",
    "encoding, layer_to_class, class_to_layer = categorical_encoding(df.Layer, drop=True)\n",
    "df = df.loc[df.Layer.isin(layer_to_class.keys())].copy()\n",
    "df['X_Layer'] = encoding\n",
    "df.drop(columns=['Layer'], inplace=True)\n",
    "\n",
    "# Verification\n",
    "assert set(df.X_Layer) == set(layer_to_class.values()), 'encoding mismatch'\n",
    "\n",
    "print_feature_distribution(df.X_Layer, idx_to_key=class_to_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ff8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track & Arc Layer Categorical Feature\n",
    "dt = dt.loc[dt.Layer.isin(layer_to_class.keys())].copy()\n",
    "dt['X_Layer'] = [layer_to_class[l] for l in dt.Layer]\n",
    "dt.drop(columns=['Layer'], inplace=True)\n",
    "\n",
    "print_feature_distribution(dt.X_Layer, idx_to_key=class_to_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea81a6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# InComponent Categorical Feature\n",
    "dt['X_InCmp'] = [0 if c in [0,'0'] else 1 for c in dt.InComponent]\n",
    "dt.drop(columns=['InComponent'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345be7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_rotations(rotation, round_degree=15):\n",
    "    rotation = rotation - 360 if rotation >= 360 else rotation\n",
    "    return int(np.round(rotation/round_degree)*round_degree)\n",
    "\n",
    "df.Rotation = df.Rotation.apply(round_rotations)\n",
    "print_feature_distribution(df.Rotation)\n",
    "\n",
    "orig_len = len(df)\n",
    "\n",
    "encoding, rotation_to_class, class_to_rotation = categorical_encoding(df.Rotation, drop=False, thresh=0.001)\n",
    "#df = df.loc[df.Rotation.isin(rotation_to_class.keys())].copy()\n",
    "df['XY_ROT'] = encoding\n",
    "df.drop(columns=['Rotation'], inplace=True)\n",
    "\n",
    "#print(f'Dropped {orig_len - len(df)} rows w/ infrequent rotations.')\n",
    "print_feature_distribution(df.XY_ROT, idx_to_key=class_to_rotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7db142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate silk and component types in to their own dataframes\n",
    "df_s = df.copy().loc[df.Type == 'slk-des']\n",
    "df_c = df.copy().loc[df.Type == 'cmp']\n",
    "\n",
    "# Silk Rotation is an output feature\n",
    "df_s.rename(columns={'XY_ROT': 'Y_ROT'}, inplace=True)\n",
    "\n",
    "# Cmp Layer & Rotation are input features instead of Output Labels\n",
    "df_c.rename(columns={'XY_ROT': 'X_ROT'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee32c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_len = len(df_s)\n",
    "\n",
    "# Verification\n",
    "assert set(df_s.Board) == set(df_c.Board), 'Missing data.'\n",
    "\n",
    "# Remove rows where designator is not in both silkscreen & component datasets\n",
    "for b in set(df_s.Board):\n",
    "    s_des = set(df_s[df_s.Board==b].Designator)\n",
    "    c_des = set(df_c[df_c.Board==b].Designator)\n",
    "    \n",
    "    rem = s_des.difference(c_des)\n",
    "    df_s = df_s.drop(df_s[(df_s.Board==b) & (df_s.Designator.isin(rem))].index)\n",
    "    \n",
    "    if len(rem) > 0:\n",
    "        print(f'Board: {b}, {data_files[b]}')\n",
    "        print(rem)\n",
    "        \n",
    "print(f'Removed {orig_len-len(df_s)} rows. Best case is if this is 0.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0ef71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split track & arc datasets\n",
    "da = dt.loc[dt.Type == 'slk-arc']\n",
    "dt = dt.loc[dt.Type == 'slk-trk']\n",
    "\n",
    "drops = [c for c in ['Length','Type','LineType'] if c in da.columns]\n",
    "da.drop(columns=drops, inplace=True) # Remove track specific data from arc dataframe\n",
    "drops = [c for c in ['StartAngle','EndAngle','Radius','Type','isCircle','isClockwise'] if c in dt.columns]\n",
    "dt.drop(columns=drops, inplace=True) # ditto for arc data\n",
    "\n",
    "# Set cadence start/end angles & length to 0 since cadence doesn't have an equivalent. \n",
    "# This is a temporary solution since this data isn't currently being used anyway\n",
    "if 'StartAngle' in da.columns:\n",
    "    da.loc[(da.StartAngle == 'NA'), 'StartAngle'] = 0\n",
    "    da.loc[(da.EndAngle == 'NA'), 'EndAngle'] = 0\n",
    "    dt.loc[(dt.Length == 'NA'), 'Length'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaace1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create node indexes\n",
    "for b in set(df.Board):\n",
    "    df_s.loc[df_s.Board==b, 'idx'] = [i for i in range(len(df_s.loc[df_s.Board==b]))]\n",
    "    df_c.loc[df_c.Board==b, 'idx'] = [i for i in range(len(df_c.loc[df_c.Board==b]))]\n",
    "    da.loc[da.Board == b, 'idx'] = [i for i in range(len(da.loc[da.Board==b]))]\n",
    "    dt.loc[dt.Board == b, 'idx'] = [i for i in range(len(dt.loc[dt.Board==b]))]\n",
    "    \n",
    "df_s.idx = df_s.idx.astype(int)\n",
    "df_c.idx = df_c.idx.astype(int)\n",
    "da.idx = da.idx.astype(int)\n",
    "dt.idx = dt.idx.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d6440c",
   "metadata": {},
   "source": [
    "### Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28033336",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCALER = 10000\n",
    "\n",
    "# Component Data\n",
    "for c in ['x','y','L','R','T','B']:\n",
    "    df_c[c] = df_c[c]/SCALER\n",
    "\n",
    "# Silkscreen Data\n",
    "df_s['W'] = abs(df_s['R'] - df_s['L'])/SCALER\n",
    "df_s['H'] = abs(df_s['T'] - df_s['B'])/SCALER\n",
    "df_s.drop(columns=['L','R','T','B','Type'], inplace=True)\n",
    "\n",
    "# Track Data\n",
    "for c in ['x','y','x1','x2','y1','y2','Width','Length']:\n",
    "    if c in dt.columns:\n",
    "        dt[c] = dt[c].astype(float)/SCALER\n",
    "    \n",
    "# Arc Data\n",
    "for c in ['x','y','x1','x2','y1','y2','Width','Radius']:\n",
    "    if c in da.columns:\n",
    "        da[c] = da[c].astype(float)/SCALER\n",
    "\n",
    "if 'StartAngle' in da.columns:\n",
    "    da.StartAngle = da.StartAngle.astype(float)/360\n",
    "    da.EndAngle = da.EndAngle.astype(float)/360\n",
    "\n",
    "# Not sure if it's better to scale the output predictions\n",
    "# df_s['x'] = df_s['x']/SCALER\n",
    "# df_s['y'] = df_s['y']/SCALER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4bd0f2",
   "metadata": {},
   "source": [
    "### Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11b47b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmp_features = ['X_Tool','X_DES','X_Layer','X_ROT','x','y','L','R','T','B']\n",
    "slk_features = ['X_Tool','X_DES','W','H']\n",
    "trk_features = ['X_Tool','X_DES','X_Layer','x','y','x1','x2','y1','y2','Width']\n",
    "arc_features = ['X_Tool','X_DES','X_Layer','x','y','x1','x2','y1','y2','Width','StartAngle','EndAngle','Radius']\n",
    "slk_lbls = ['Y_ROT','x','y']\n",
    "\n",
    "# Categorical Pin Features\n",
    "categorical_cmp_features = [c for c in df_c.columns if 'int' in str(df_c[c].dtype) and c in cmp_features]\n",
    "categorical_slk_features = [c for c in df_s.columns if 'int' in str(df_s[c].dtype) and c in slk_features]\n",
    "categorical_trk_features = [c for c in dt.columns if 'int' in str(dt[c].dtype) and c in trk_features]\n",
    "categorical_arc_features = [c for c in da.columns if 'int' in str(da[c].dtype) and c in arc_features]\n",
    "\n",
    "# Non-Categorical Features\n",
    "continuous_cmp_features = [c for c in df_c.columns if c not in categorical_cmp_features and c in cmp_features]\n",
    "continuous_slk_features = [c for c in df_s.columns if c not in categorical_slk_features and c in slk_features]\n",
    "continuous_trk_features = [c for c in dt.columns if c not in categorical_trk_features and c in trk_features]\n",
    "continuous_arc_features = [c for c in da.columns if c not in categorical_arc_features and c in arc_features]\n",
    "\n",
    "assert all([cmp_features[i] in categorical_cmp_features for i, f in enumerate(categorical_cmp_features)]), 'Categorical Features must be first in the feature columns followed by numerical features'\n",
    "assert all([slk_features[i] in categorical_slk_features for i, f in enumerate(categorical_slk_features)]), 'Categorical Features must be first in the feature columns followed by numerical features'\n",
    "assert all([trk_features[i] in categorical_trk_features for i, f in enumerate(categorical_trk_features)]), 'Categorical Features must be first in the feature columns followed by numerical features'\n",
    "assert all([arc_features[i] in categorical_arc_features for i, f in enumerate(categorical_arc_features)]), 'Categorical Features must be first in the feature columns followed by numerical features'\n",
    "\n",
    "# Class count for each categorical feature\n",
    "categorical_dict = {c: len(set(df_c[c])) for c in categorical_cmp_features}\n",
    "categorical_dict.update({c: len(set(df_s[c])) for c in categorical_slk_features}) \n",
    "\n",
    "print('Please verify the following are continuous features:')\n",
    "print(f'Component Features: {continuous_cmp_features}')\n",
    "print(f'Silk Features: {continuous_slk_features}')\n",
    "print(f'Track Features: {continuous_trk_features}')\n",
    "print(f'Arc Features: {continuous_arc_features}')\n",
    "print('\\n...and the following are categorical features:')\n",
    "print(f'Component Features: {categorical_cmp_features}')\n",
    "print(f'Silk Features: {categorical_slk_features}')\n",
    "print(f'Track Features: {categorical_trk_features}')\n",
    "print(f'Arc Features: {categorical_arc_features}')\n",
    "\n",
    "print()\n",
    "print('Categorical Dictionary: ', categorical_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c095de5",
   "metadata": {},
   "source": [
    "### Export preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aaac4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing = {'tool_to_class': tool_to_class,\n",
    "                 'class_to_tool': class_to_tool,\n",
    "                 'rotation_to_class':rotation_to_class,\n",
    "                 'class_to_rotation':class_to_rotation, \n",
    "                 'refdes_to_class': refdes_to_class, \n",
    "                 'class_to_refdes':class_to_refdes, \n",
    "                 'layer_to_class':layer_to_class, \n",
    "                 'class_to_layer':class_to_layer, \n",
    "                 'categorical_dict':categorical_dict,\n",
    "                 'cmp_features':cmp_features,\n",
    "                 'slk_features':slk_features,\n",
    "                 'trk_features':trk_features,\n",
    "                 'arc_features':arc_features,\n",
    "                 'slk_lbls':slk_lbls,\n",
    "                 'categorical_cmp_features':categorical_cmp_features,\n",
    "                 'categorical_slk_features':categorical_slk_features,\n",
    "                 'categorical_trk_features':categorical_trk_features,\n",
    "                 'categorical_arc_features':categorical_arc_features,\n",
    "                 'scaler': SCALER} \n",
    "\n",
    "\n",
    "pickle.dump(preprocessing, open('preprocessing.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdb5ec7",
   "metadata": {},
   "source": [
    "### Create node list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a126524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "good = []\n",
    "for b in set(df.Board):\n",
    "    good += [all([i == idx for i, idx in enumerate(df_s.loc[(df_s.Board==b),'idx'])])]\n",
    "    good += [all([i == idx for i, idx in enumerate(df_c.loc[(df_c.Board==b),'idx'])])]\n",
    "\n",
    "assert all(good), 'Non continguous index'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413f4c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph import get_cmp_edge_idx, get_cmp_slk_edge_idx, get_split_mask, get_cmp_trk_edge_idx, get_trk_edge_idx\n",
    "from tqdm import tqdm\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "LOAD_DATASET = True\n",
    "dataset_filename = 'dataset.pkl'\n",
    "\n",
    "def create_graph(s, c, t):\n",
    "    data = HeteroData()\n",
    "\n",
    "    # Define Nodes\n",
    "    data['cmp'].x = torch.tensor(c[cmp_features].values, dtype=torch.float)\n",
    "    data['slk'].x = torch.tensor(s[slk_features].values, dtype=torch.float)\n",
    "    data['trk'].x = torch.tensor(t[trk_features].values, dtype=torch.float)\n",
    "    #data['arc'].x = torch.tensor(a[arc_features].values, dtype=torch.float)\n",
    "    data['slk'].y = torch.tensor(s[slk_lbls].values, dtype=torch.float)\n",
    "\n",
    "    # Define Edges\n",
    "    data['cmp','cmp-slk','slk'].edge_index = get_cmp_slk_edge_idx(c, s, ['cmp','cmp-slk','slk'])\n",
    "    data['cmp','cmp-cmp','cmp'].edge_index = get_cmp_edge_idx(c, n=5)\n",
    "    data['cmp','cmp-trk','trk'].edge_index = get_cmp_trk_edge_idx(c, t, ['cmp','cmp-trk','trk'])\n",
    "    data['trk','trk-trk','trk'].edge_index = get_trk_edge_idx(t)\n",
    "    #data['cmp','cmp-arc','arc'].edge_index = get_cmp_trk_edge_idx(c, a, ['cmp','cmp-arc','arc'])\n",
    "    \n",
    "    return data\n",
    "\n",
    "if LOAD_DATASET and os.path.exists(dataset_filename):\n",
    "    dataset, test_dataset = pickle.load(open(dataset_filename, 'rb'))\n",
    "else:\n",
    "    dataset, test_dataset = [], []\n",
    "    for b in tqdm(set(df.Board)):\n",
    "        s = df_s.loc[df_s.Board==b] # Silkscreen Data For given Board b\n",
    "        c = df_c.loc[df_c.Board==b] # Component Data For given Board b\n",
    "        t = dt.loc[dt.Board==b] # Track Data For given Board b\n",
    "        #a = da.loc[da.Board==b] # Arc Data For given Board b\n",
    "        \n",
    "        if data_files[b] in test_files:\n",
    "            # Test dataset (make predictions on this data and export to csv)\n",
    "            test_dataset += [create_graph(s, c, t)]\n",
    "        else:\n",
    "            # Train dataset\n",
    "            dataset += [create_graph(s, c, t)]\n",
    "    \n",
    "    pickle.dump((dataset, test_dataset), open(dataset_filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ba1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verification\n",
    "for data in dataset:\n",
    "    slk_len = data.x_dict['slk'].shape[0]\n",
    "    cmp_len = data.x_dict['cmp'].shape[0]\n",
    "    max_cmp_idx = max(data.edge_index_dict[('cmp','cmp-slk','slk')][0].tolist())\n",
    "    max_slk_idx = max(data.edge_index_dict[('cmp','cmp-slk','slk')][1].tolist())\n",
    "    assert max_slk_idx < slk_len, 'Silk Error'\n",
    "    assert max_cmp_idx < cmp_len, 'Cmp Error'\n",
    "    \n",
    "    max_cmp_idx1 = max(data.edge_index_dict[('cmp','cmp-cmp','cmp')][0].tolist())\n",
    "    max_cmp_idx2 = max(data.edge_index_dict[('cmp','cmp-cmp','cmp')][1].tolist())\n",
    "    assert max_cmp_idx1 < cmp_len, 'Cmp Error2'\n",
    "    assert max_cmp_idx2 < cmp_len, 'Cmp Error3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4ffadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric import seed_everything\n",
    "\n",
    "seed_everything(457301994)\n",
    "\n",
    "train_loader = DataLoader(dataset[1:], batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(dataset[:1], batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d9d337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model defined in external file\n",
    "from model import model, gnn_type, actFunc, dropout, gnn_channels, hidden_channels\n",
    "\n",
    "gnn_str = ''.join([c for c in str(gnn_type).split('.')[-1] if c.isalnum()])\n",
    "act_str = ''.join([c for c in str(actFunc).split('.')[-1] if c.isalnum()])\n",
    "layers = f'lyrs{str(model).count(\"HeteroConv\")}'\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec4084a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_MODEL = False # Meaning load trained model\n",
    "MODEL_SAVE_DIR = 'models'\n",
    "MODEL_SAVE_NAME = 'model.pth'\n",
    "if not os.path.isdir(MODEL_SAVE_DIR):\n",
    "    os.mkdir(MODEL_SAVE_DIR)\n",
    "model_save_path = os.path.join(MODEL_SAVE_DIR, MODEL_SAVE_NAME)\n",
    "state_dict_path = os.path.join(MODEL_SAVE_DIR, 'state_dict.pth')\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model = torch.load(model_save_path)\n",
    "    model.load_state_dict(torch.load(state_dict_path)['model_state_dict'])\n",
    "    \n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a629c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.0001\n",
    "wd = 0.01\n",
    "note = f'wd{wd}'\n",
    "\n",
    "model_key = f'{gnn_str}_{lr}_{len(dataset)}_{act_str}_{dropout}_{gnn_channels}_{hidden_channels}_{layers}_{note}'.strip('_')        \n",
    "print('Model Name:')\n",
    "print(model_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ec706",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from predict import export_predictions\n",
    "import torch.nn.functional as F\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)  # Define optimizer.\n",
    "\n",
    "EXPORT_PREDICTIONS = True # If true, make sure you have pcb datasets in the data/test/ directory\n",
    "\n",
    "# Create dataframe from only test_data boards (pass to export_predictions)\n",
    "test_board_idxs = [b for b in set(df_s.Board) if data_files[b] in test_files]\n",
    "pr = df_s.loc[df_s.Board.isin(test_board_idxs)].copy()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for d in train_loader:  # Iterate over each mini-batch.\n",
    "        d = d.to(device)\n",
    "\n",
    "        _, rot_out, dx_out, dy_out = model(d.x_dict, d.edge_index_dict)  # Perform a single forward pass.\n",
    "        dx_out = dx_out.reshape((-1,))\n",
    "        dy_out = dy_out.reshape((-1,))\n",
    "        \n",
    "        rot_loss = F.cross_entropy(rot_out, d['slk'].y[:, slk_lbls.index('Y_ROT')].to(dtype=torch.long))\n",
    "        \n",
    "        dx_loss = F.mse_loss(dx_out, d['slk'].y[:, slk_lbls.index('x')])\n",
    "        dy_loss = F.mse_loss(dy_out, d['slk'].y[:, slk_lbls.index('y')])\n",
    "\n",
    "        loss = rot_loss + dx_loss + dy_loss\n",
    "        total_loss += float(loss)\n",
    "\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "def test(mode='test'):\n",
    "    model.eval()\n",
    "    \n",
    "    total_x_loss, total_y_loss, total_rot_loss = 0,0,0\n",
    "    total_loss = 0\n",
    "    for d in val_loader:\n",
    "        d = d.to(device)\n",
    "        \n",
    "        embedding, rot_out, dx_out, dy_out = model(d.x_dict, d.edge_index_dict)\n",
    "        dx_out = dx_out.reshape((-1,))\n",
    "        dy_out = dy_out.reshape((-1,))\n",
    "\n",
    "        dx_loss = F.mse_loss(dx_out, d['slk'].y[:, slk_lbls.index('x')])\n",
    "        dy_loss = F.mse_loss(dy_out, d['slk'].y[:, slk_lbls.index('y')])\n",
    "        \n",
    "        rot_loss = F.cross_entropy(rot_out, d['slk'].y[:, slk_lbls.index('Y_ROT')].to(dtype=torch.long))\n",
    "        \n",
    "        loss = rot_loss + dx_loss + dy_loss\n",
    "        \n",
    "        \n",
    "        total_x_loss += float(dx_loss)\n",
    "        total_y_loss += float(dy_loss)\n",
    "        total_rot_loss += float(rot_loss)\n",
    "        total_loss += float(loss)\n",
    "    \n",
    "    return embedding, total_loss, total_rot_loss, total_x_loss, total_y_loss\n",
    "\n",
    "loss_list = []\n",
    "val_list = []\n",
    "rot_loss_list = []\n",
    "dx_loss_list = []\n",
    "dy_loss_list = []\n",
    "times = []\n",
    "start_time = time()\n",
    "\n",
    "if os.path.exists('model_compare.pkl'):\n",
    "    model_dict = pickle.load(open('model_compare.pkl', 'rb'))\n",
    "else:\n",
    "    model_dict = {}\n",
    "    \n",
    "for epoch in range(1, 200000000):\n",
    "    loss = train()\n",
    "    \n",
    "    if epoch % 1 == 0:\n",
    "        embeddings, val_loss, rot_loss, dx_loss, dy_loss = test('val') \n",
    "        \n",
    "        if np.isnan(rot_loss) or np.isnan(val_loss):\n",
    "            break\n",
    "        \n",
    "        loss_list += [loss]\n",
    "        val_list += [val_loss]\n",
    "        rot_loss_list += [rot_loss]\n",
    "        dx_loss_list += [dx_loss]\n",
    "        dy_loss_list += [dy_loss]\n",
    "        \n",
    "        times += [time() - start_time]\n",
    "    \n",
    "        print(f'Epoch: {epoch:03d}, Loss: {loss:.0f}, Val: {val_loss:.0f}, Rot Loss: {rot_loss: .4f}')\n",
    "        \n",
    "    if epoch % 100 == 0:\n",
    "        if EXPORT_PREDICTIONS and len(test_files) > 0:\n",
    "            export_predictions(test_loader, model, pr, device=device, class_to_rotation=class_to_rotation,\n",
    "                               export_filename=f'predictions_{epoch}.csv', \n",
    "                               export_subdir='data//predictions')\n",
    "        \n",
    "        model_dict[model_key] = (loss_list, val_list, (dx_loss_list, dy_loss_list), times)\n",
    "        pickle.dump(model_dict, open('model_compare.pkl', 'wb'))\n",
    "        \n",
    "        pickle.dump({'loss': loss_list, \n",
    "                     'val': val_list,\n",
    "                     'rot_loss': rot_loss_list,\n",
    "                     'dx_loss': dx_loss_list,\n",
    "                     'dy_loss': dy_loss_list,\n",
    "                     'embeddings': [e.cpu() for e in embeddings]},\n",
    "                    open('log_batch.pkl', 'wb'))\n",
    "        \n",
    "        torch.save(model, model_save_path)\n",
    "        torch.save({'epoch': epoch, 'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(), 'loss': loss}, state_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf25975",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
